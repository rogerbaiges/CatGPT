{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating the training corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This document downloads the most popular catalan datasets from projecte-aina and creates a training corpus with them. The datasets are:\n",
    "- Oscar +5GB\n",
    "- Catalan_textual_corpus +10GB\n",
    "\n",
    "Although they are one of the best options for training and LLM it is still not enough filtered and preprocessed so after downloaded texts, will be\n",
    "trying to create a better dataset for training. This is crucial for small LLM models as they need to be trained with a good dataset to perform well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# Load the dataset\n",
    "dataset = load_dataset('projecte-aina/catalan_general_crawling', trust_remote_code=True)\n",
    "\n",
    "# Extract the 'train' split and preprocess\n",
    "corpus_text = \" \".join(dataset['train']['text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Storing different data sizes from the full dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def limit_dataset_size(corpus_text, size_mb):\n",
    "    max_bytes = size_mb * 1024 * 1024  # Convert MB to bytes\n",
    "    encoded_text = corpus_text.encode('utf-8')\n",
    "    limited_text = encoded_text[:max_bytes].decode('utf-8', errors='ignore')\n",
    "    return limited_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def saving_text_to_file(text, filename):\n",
    "    with open(f\"../data/{filename}.txt\", \"w\") as f:\n",
    "        f.write(text)\n",
    "\n",
    "\n",
    "# Limit the dataset size to X MB\n",
    "corpus_text = limit_dataset_size(corpus_text, 10)\n",
    "saving_text_to_file(corpus_text, \"tiny_corpus\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the function to save dataset texts to a .txt file\n",
    "def save_text_to_file(texts, file_path):\n",
    "    with open(file_path, \"w\") as f:\n",
    "        for line in texts:\n",
    "            f.write(line.replace(\"\\n\", \" \") + \"\\n\")  # Replace newlines within articles to maintain proper formatting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargar el dataset original\n",
    "with open('../data/catalan_oscar.txt', 'r', encoding='utf-8') as f:\n",
    "    corpus_text = f.read()\n",
    "\n",
    "# Limitar el tama√±o del dataset a 50 MB\n",
    "limited_text = limit_dataset_size(corpus_text, 50)\n",
    "saving_text_to_file(limited_text, \"small_catalan_oscar\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the OSCAR dataset for Catalan\n",
    "oscar_dataset = load_dataset(\"oscar\", \"unshuffled_deduplicated_ca\", split=\"train\")\n",
    "oscar_text = [example['text'] for example in oscar_dataset]\n",
    "save_text_to_file(oscar_text, \"catalan_oscar.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import bz2\n",
    "import xml.etree.ElementTree as ET\n",
    "from datasets import load_dataset\n",
    "\n",
    "# Define the function to save dataset texts to a .txt file\n",
    "def save_text_to_file(texts, file_path):\n",
    "    with open(file_path, \"w\") as f:\n",
    "        for line in texts:\n",
    "            f.write(line.replace(\"\\n\", \" \") + \"\\n\")  # Replace newlines within articles to maintain proper formatting\n",
    "\n",
    "# Function to download and extract Wikipedia dump\n",
    "def download_wikipedia_dump(url, output_file):\n",
    "    response = requests.get(url)\n",
    "    with open(output_file, 'wb') as file:\n",
    "        file.write(response.content)\n",
    "    print(f\"Downloaded Wikipedia dump to {output_file}\")\n",
    "\n",
    "def parse_wikipedia_dump(dump_file):\n",
    "    with bz2.open(dump_file, 'rt') as f:\n",
    "        context = ET.iterparse(f, events=('end',))\n",
    "        for event, elem in context:\n",
    "            if elem.tag.endswith('text'):\n",
    "                yield elem.text\n",
    "            elem.clear()\n",
    "\n",
    "# Download Wikipedia dump\n",
    "wikipedia_dump_url = 'https://dumps.wikimedia.org/cawiki/latest/cawiki-latest-pages-articles.xml.bz2'\n",
    "wikipedia_dump_file = 'cawiki-latest-pages-articles.xml.bz2'\n",
    "download_wikipedia_dump(wikipedia_dump_url, wikipedia_dump_file)\n",
    "\n",
    "# Parse Wikipedia dump\n",
    "wikipedia_texts = list(parse_wikipedia_dump(wikipedia_dump_file))\n",
    "save_text_to_file(wikipedia_texts, \"catalan_wikipedia.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess data by removing new lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty lines have been removed and the result has been saved in ../data/CatGPT_dataset.txt.\n"
     ]
    }
   ],
   "source": [
    "# Name of the original file and the output file\n",
    "input_filename = '../data/catalan_textual_corpus.txt'\n",
    "output_filename = '../data/CatGPT_dataset.txt'\n",
    "\n",
    "# Open the original file in read mode and the output file in write mode\n",
    "with open(input_filename, 'r') as infile, open(output_filename, 'w') as outfile:\n",
    "    for line in infile:\n",
    "        # Strip to remove leading and trailing whitespaces and newline characters\n",
    "        if line.strip():\n",
    "            outfile.write(line)\n",
    "\n",
    "print(f\"Empty lines have been removed and the result has been saved in {output_filename}.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PATUFET DATASET"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After using the previous datasets, the model was fine-tuned with the patufet-textbooks dataset. This datasets is a collection of synthetic catalan texts created by using a much larger LLM, gemini-flash. It is structured in many differents fields and topics like Mathematics, History, Geography, etc. Moreover it is explained for different types of audience like kids, general persons or even researchers. The dataset is available in the huggingface datasets library and can be downloaded by using the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# Load the dataset from Hugging Face\n",
    "dataset = load_dataset(\"pauhidalgoo/patufet-textbooks\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# C4 CATALAN DATASET"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This part of the script aims to create one of the bests dataset for the Catalan language. The dataset will be created by translating the well-known\n",
    "C4 in its variant realnewslike dataset to Catalan resulting in around 25 GB of text data about news articles. Its motivation\n",
    "comes from the lack of Catalan datasets in the NLP community and the need to have a good dataset to train models in this language."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset is downloaded by creating batches of 1 GB (parameter) in order to avoid memory issuess during the translation and training process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets\n",
    "\n",
    "class C4NewsBatchLoader:\n",
    "    def __init__(self, split='train'):\n",
    "        \"\"\"\n",
    "        Initialize the C4NewsBatchLoader with a specific split.\n",
    "        \n",
    "        Parameters:\n",
    "        split (str): The split of the dataset to load (train, validation, test).\n",
    "        \"\"\"\n",
    "        self.dataset = datasets.load_dataset('allenai/c4', 'realnewslike', split=split, streaming=True)\n",
    "        self.dataset_iter = iter(self.dataset)\n",
    "        self.bytes_written = 0\n",
    "        self.file_count = 1\n",
    "\n",
    "    def _save_to_file(self, text, file_count):\n",
    "        \"\"\"\n",
    "        Save text to a file.\n",
    "        \n",
    "        Parameters:\n",
    "        text (str): The text to save.\n",
    "        file_count (int): The current file count for naming the file.\n",
    "        \"\"\"\n",
    "        file_name = f\"../data/CA_realnewslike{file_count}.txt\"\n",
    "        print(f\"Saving to {file_name}\")\n",
    "        with open(file_name, 'w', encoding='utf-8') as f:\n",
    "            f.write(text)\n",
    "\n",
    "    def split_to_files(self, max_size_gb=1):\n",
    "        \"\"\"\n",
    "        Split the dataset into files of approximately max_size_gb GB each.\n",
    "        \n",
    "        Parameters:\n",
    "        max_size_gb (int): The maximum size of each file in GB.\n",
    "        \"\"\"\n",
    "        max_size_bytes = max_size_gb * 1024**3  # Convert GB to bytes\n",
    "        current_text = []\n",
    "\n",
    "        try:\n",
    "            while True:\n",
    "                example = next(self.dataset_iter)\n",
    "                text = example['text']\n",
    "                current_text.append(text)\n",
    "                self.bytes_written += len(text.encode('utf-8'))\n",
    "\n",
    "                if self.bytes_written >= max_size_bytes:\n",
    "                    self._save_to_file(''.join(current_text), self.file_count)\n",
    "                    self.file_count += 1\n",
    "                    current_text = []\n",
    "                    self.bytes_written = 0\n",
    "\n",
    "        except StopIteration:\n",
    "            if current_text:\n",
    "                self._save_to_file(''.join(current_text), self.file_count)\n",
    "\n",
    "# Example usage:\n",
    "batch_loader = C4NewsBatchLoader(split='train')\n",
    "batch_loader.split_to_files(max_size_gb=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TRANSLATE DATA USING LOCAL TRANSFORMERS MODEL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At the end it was not possible to translate the whole dataset using the local transformers model due to computational and economic limitations. The script is still available as it works and can be used to translate smaller datasets. If having a powerful machine, it is possible to translate the whole dataset by changing the `batch_size` parameter to a higher value. If you do so, please let me know how it went! If possible it would be much better to use an LLM for translating with a better quality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import torch\n",
    "from transformers import MarianMTModel, MarianTokenizer\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "# Download the NLTK data needed for sentence tokenization\n",
    "nltk.download('punkt')\n",
    "\n",
    "def translate_sentences(sentences, model, tokenizer):\n",
    "    inputs = tokenizer(sentences, return_tensors=\"pt\", padding=True, truncation=True, max_length=512)\n",
    "    inputs = {key: value.to('cuda') for key, value in inputs.items()}  # Move inputs to GPU\n",
    "    translated = model.generate(**inputs)\n",
    "    translated_texts = tokenizer.batch_decode(translated, skip_special_tokens=True)\n",
    "    return translated_texts\n",
    "\n",
    "def translate_file(input_path, output_path, chunk_size=1024*1024, batch_size=32):  # 1MB chunk size, batch size 32\n",
    "    model_name = \"Helsinki-NLP/opus-mt-en-ca\"\n",
    "    tokenizer = MarianTokenizer.from_pretrained(model_name)\n",
    "    model = MarianMTModel.from_pretrained(model_name)\n",
    "    model.to('cuda')  # Move model to GPU\n",
    "\n",
    "    with open(input_path, 'r', encoding='utf-8') as infile, open(output_path, 'w', encoding='utf-8') as outfile:\n",
    "        while True:\n",
    "            text_chunk = infile.read(chunk_size)\n",
    "            if not text_chunk:\n",
    "                break\n",
    "\n",
    "            sentences = sent_tokenize(text_chunk)\n",
    "            translated_sentences = []\n",
    "            for i in range(0, len(sentences), batch_size):\n",
    "                batch = sentences[i:i + batch_size]\n",
    "                translated_batch = translate_sentences(batch, model, tokenizer)\n",
    "                translated_sentences.extend(translated_batch)\n",
    "\n",
    "            translated_text = \" \".join(translated_sentences)\n",
    "            outfile.write(translated_text + \"\\n\")\n",
    "            print(\"Translated and wrote a chunk.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
