{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DEFINING TRAINING PARAMETERS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from CatGPT_model import GPT, GPTConfig\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from tokenizers import ByteLevelBPETokenizer\n",
    "from time import time\n",
    "from dataclasses import dataclass\n",
    "from math import cos, pi\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define checkpoint path\n",
    "checkpoint_path = \"../models/checkpoint.pth\"\n",
    "\n",
    "# Function to save checkpoint\n",
    "def save_checkpoint(model, optimizer, step, dataloader, checkpoint_path):\n",
    "    checkpoint = {\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'step': step,\n",
    "        'dataloader_state': {\n",
    "            'current_position': dataloader.file_pointer.tell(),\n",
    "            'tokens': dataloader.tokens\n",
    "        }\n",
    "    }\n",
    "    torch.save(checkpoint, checkpoint_path)\n",
    "    print(f\"Checkpoint saved at step {step}\")\n",
    "\n",
    "# Function to load checkpoint\n",
    "def load_checkpoint(model, optimizer, dataloader, checkpoint_path, device):\n",
    "    checkpoint = torch.load(checkpoint_path, map_location=device)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "    dataloader.current_position = checkpoint['dataloader_state']['current_position']\n",
    "    dataloader.tokens = checkpoint['dataloader_state']['tokens']\n",
    "    dataloader.file_pointer.seek(dataloader.current_position)\n",
    "    step = checkpoint['step']\n",
    "    print(f\"Checkpoint loaded from step {step}\")\n",
    "    return step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Data Loadet class\n",
    "\n",
    "class DataLoaderLite:\n",
    "    def __init__(self, file, B, T, buffer_size=100000000, device='cpu'):\n",
    "        self.file = file\n",
    "        self.B = B\n",
    "        self.T = T\n",
    "        self.buffer_size = buffer_size\n",
    "        self.device = device\n",
    "        self.current_position = 0\n",
    "        self.tokenizer = ByteLevelBPETokenizer(\n",
    "            '../tokenizer/vocab.json',\n",
    "            '../tokenizer/merges.txt'\n",
    "        )\n",
    "        self.tokens = torch.tensor([], dtype=torch.long, device=self.device)\n",
    "        self.file_pointer = open(self.file, 'r')\n",
    "\n",
    "    def _load_tokens(self):\n",
    "        text = self.file_pointer.read(self.buffer_size)\n",
    "        if not text:\n",
    "            print(f\"TOTAL EPOCH OF TEXT FINISHED\")\n",
    "            self.file_pointer.seek(0)  # Reset to the beginning if end is reached\n",
    "            text = self.file_pointer.read(self.buffer_size)\n",
    "        encoded = self.tokenizer.encode(text).ids\n",
    "        return torch.tensor(encoded, dtype=torch.long, device=self.device)\n",
    "\n",
    "    def next_batch(self):\n",
    "        B, T = self.B, self.T\n",
    "        while len(self.tokens) <= B * T:\n",
    "            self.tokens = torch.cat((self.tokens, self._load_tokens()), dim=0)\n",
    "\n",
    "        buf = self.tokens[:B * T + 1]\n",
    "        self.tokens = self.tokens[B * T + 1:]  # Discard used tokens\n",
    "\n",
    "        x = buf[:-1].view(B, T)  # inputs\n",
    "        y = buf[1:].view(B, T)   # targets\n",
    "        return x, y\n",
    "\n",
    "    def save_state(self, path):\n",
    "        state = {\n",
    "            'current_position': self.file_pointer.tell(),\n",
    "            'tokens': self.tokens\n",
    "        }\n",
    "        torch.save(state, path)\n",
    "        print(f\"DataLoaderLite state saved at {path}\")\n",
    "\n",
    "    def load_state(self, path):\n",
    "        state = torch.load(path)\n",
    "        self.current_position = state['current_position']\n",
    "        self.tokens = state['tokens'].to(self.device)\n",
    "        self.file_pointer.seek(self.current_position)\n",
    "        print(f\"DataLoaderLite state loaded from {path}\")\n",
    "\n",
    "    def close(self):\n",
    "        self.file_pointer.close()\n",
    "\n",
    "    def reset(self, new_file):\n",
    "        self.file_pointer.close()  # Close the current file\n",
    "        self.file = new_file\n",
    "        self.file_pointer = open(self.file, 'r')  # Open the new file\n",
    "        self.current_position = 0\n",
    "        self.tokens = torch.tensor([], dtype=torch.long, device=self.device)\n",
    "        print(f\"DataLoaderLite reset with new file {new_file}\")\n",
    "\n",
    "@dataclass\n",
    "class CatGPT_training_config:\n",
    "    B = 2\n",
    "    T = 1024\n",
    "    total_batch_size = 524288\n",
    "    float_matmul_precision = 'high'\n",
    "    vocab_size = 32768\n",
    "    max_lr = 6e-4\n",
    "    min_lr = max_lr * 0.1\n",
    "    warmup_steps = 35\n",
    "    steps = 10000\n",
    "    weight_decay = 0.1\n",
    "    betas = (0.9, 0.95)\n",
    "    eps = 1e-8\n",
    "    compile_model = True\n",
    "    use_gpu = False\n",
    "\n",
    "CatGPT_basic_config = CatGPT_training_config()\n",
    "\n",
    "assert (CatGPT_basic_config.total_batch_size % (CatGPT_basic_config.B * CatGPT_basic_config.T)) == 0, \"make sure total_batch_size is divisible by B * T\"\n",
    "grad_accum_steps = CatGPT_basic_config.total_batch_size // (CatGPT_basic_config.B * CatGPT_basic_config.T)\n",
    "print(f\"total desired batch size: {CatGPT_basic_config.total_batch_size}\")\n",
    "print(f\"=> calculated gradient accumulation steps: {grad_accum_steps}\")\n",
    "\n",
    "\n",
    "device = \"cpu\"\n",
    "\n",
    "if CatGPT_training_config.use_gpu:\n",
    "    if torch.cuda.is_available():\n",
    "        device = \"cuda\"\n",
    "    elif hasattr(torch.backends, \"mps\") and torch.backends.mps.is_available():\n",
    "        device = \"mps\"\n",
    "    print(f\"Using device: {device}\")\n",
    "\n",
    "\n",
    "# Create DataLoader\n",
    "train_loader = DataLoaderLite(\"../data/tiny_corpus.txt\", B=CatGPT_training_config.B, T=CatGPT_training_config.T)\n",
    "\n",
    "# Set matmul precision to lower\n",
    "\n",
    "torch.set_float32_matmul_precision(CatGPT_training_config.float_matmul_precision)\n",
    "\n",
    "# Create model and optimizer\n",
    "model = GPT(GPTConfig(vocab_size=CatGPT_training_config.vocab_size))\n",
    "model.to(device)\n",
    "\n",
    "if CatGPT_training_config.compile_model:\n",
    "    model = torch.compile(model)\n",
    "\n",
    "# Define optimizer\n",
    "optimizer = model.configure_optimizers(weight_decay=CatGPT_training_config.weight_decay, learning_rate=CatGPT_training_config.max_lr, device=device)\n",
    "\n",
    "# Load checkpoint if exists\n",
    "start_step = 0\n",
    "if os.path.exists(checkpoint_path):\n",
    "    start_step = load_checkpoint(model, optimizer, train_loader, checkpoint_path, device)\n",
    "\n",
    "# Warmup + cosine decay learning rate schedule\n",
    "\n",
    "def get_lr(it):\n",
    "    # 1) linear warmup for warmup_iters steps\n",
    "    if it < CatGPT_basic_config.warmup_steps:\n",
    "        return CatGPT_basic_config.max_lr * (it + 1) / CatGPT_basic_config.warmup_steps\n",
    "    # 2) if it > lr_decay_iters, return min learning rate\n",
    "    if it > CatGPT_basic_config.steps:\n",
    "        return CatGPT_basic_config.min_lr\n",
    "    # 3) in between, use cosine decay down to min learning rate\n",
    "    decay_ratio = (it - CatGPT_basic_config.warmup_steps) / (CatGPT_basic_config.steps - CatGPT_basic_config.warmup_steps)\n",
    "    assert 0 <= decay_ratio <= 1\n",
    "    coeff = 0.5 * (1.0 + cos(pi * decay_ratio)) # coeff starts at 1 and goes to 0\n",
    "    return CatGPT_basic_config.min_lr + coeff * (CatGPT_basic_config.max_lr - CatGPT_basic_config.min_lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TRAINING THE MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(start_step, CatGPT_basic_config.steps):\n",
    "    initial_time = time()\n",
    "    optimizer.zero_grad()\n",
    "    loss_accum = 0.0\n",
    "    for micro_step in range(grad_accum_steps):\n",
    "        print(micro_step)\n",
    "        x, y = train_loader.next_batch()\n",
    "        x, y = x.to(device), y.to(device)\n",
    "\n",
    "        if device == \"cuda\":\n",
    "            with torch.autocast(device_type=device, dtype=torch.bfloat16):\n",
    "                logits, loss = model(x, y)\n",
    "        else:\n",
    "            logits, loss = model(x, y)\n",
    "        loss = loss / grad_accum_steps\n",
    "        loss_accum += loss.detach()\n",
    "        loss.backward()\n",
    "    norm = torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "\n",
    "    # Update the learning rate\n",
    "    lr = get_lr(i)\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = lr\n",
    "    optimizer.step()\n",
    "    dt = time() - initial_time\n",
    "    tokens_processed = train_loader.B * train_loader.T * grad_accum_steps\n",
    "    tokens_per_second = tokens_processed / dt\n",
    "    print(f\"Step {i} | Loss: {loss_accum.item()} | Time: {dt} | Tokens/s: {tokens_per_second} | LR: {lr}\")\n",
    "\n",
    "    # Save checkpoint periodically\n",
    "    if (i + 1) % 250 == 0:\n",
    "        save_checkpoint(model, optimizer, i + 1, train_loader, checkpoint_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RESET THE TRAINING LOADER"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This must be done when changing from dataset file in order to start again from the beginning of the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader.reset('/content/drive/MyDrive/CatGPT/CatGPT/data/catalan_oscar.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SAVING THE MODEL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This script saves the model with pytorch's format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(model, model_path):\n",
    "    \"\"\"\n",
    "    Save only the model state dictionary to the specified path.\n",
    "\n",
    "    Parameters:\n",
    "    model (torch.nn.Module): The model to be saved.\n",
    "    model_path (str): The path where the model will be saved.\n",
    "    \"\"\"\n",
    "    torch.save(model.state_dict(), model_path)\n",
    "    print(f\"Model saved at {model_path}\")\n",
    "\n",
    "save_model(model, \"../models/CatGPT.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FINE-TUNING THE MODEL DIRECTLY FROM HUGGINGFACE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this part of the notebook the script in order to fine-tune the model directly from Huggingface is presented. In it we first load any dataset and split it into train and validation sets. Then labels must be encoded and padded. In the basic LLM configuration, the input_ids are the same as the labels as we try to predict the next token in the sequence. The model is then trained and evaluated on the validation set. Finally, the model is saved and can be used for inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, TrainingArguments, Trainer\n",
    "from datasets import load_dataset\n",
    "import torch\n",
    "\n",
    "\n",
    "# Load the dataset from Hugging Face\n",
    "dataset = load_dataset(\"pauhidalgoo/patufet-textbooks\")\n",
    "\n",
    "# Split the dataset into training and validation sets (99% training, 1% validation)\n",
    "dataset = dataset['train'].train_test_split(test_size=0.01)\n",
    "train_dataset = dataset['train']\n",
    "eval_dataset = dataset['test']\n",
    "\n",
    "# Load the tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"baiges/CatGPT\")\n",
    "\n",
    "# Tokenize the dataset with padding, truncation, and labels\n",
    "def tokenize_function(examples):\n",
    "    \"\"\"\n",
    "    Tokenize the text examples with the tokenizer then add padding, truncation, and return the inputs and labels.\n",
    "    \"\"\"\n",
    "    inputs = tokenizer(examples['text'], padding=\"max_length\", truncation=True, max_length=1024)\n",
    "    inputs[\"labels\"] = inputs[\"input_ids\"].copy()  # Set labels to be equal to input_ids\n",
    "    return inputs\n",
    "\n",
    "tokenized_train_dataset = train_dataset.map(tokenize_function, batched=True)\n",
    "tokenized_eval_dataset = eval_dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "# Detect the device to use (GPU, CPU, MPS)\n",
    "device = \"cpu\"\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = \"cuda\"\n",
    "elif hasattr(torch.backends, \"mps\") and torch.backends.mps.is_available():\n",
    "    device = \"mps\"\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After adapting the dataset for the Huggingface trainer it is time to define the training parameters. The training parameters are defined in the `TrainingArguments` class. The most important parameters are the number of epochs, the learning rate, the batch size, and the evaluation strategy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the pre-trained model and move it to the appropriate device\n",
    "model = AutoModelForCausalLM.from_pretrained(\"baiges/CatGPT\")\n",
    "model.to(device)\n",
    "\n",
    "# Prepare training arguments with FP16 enabled\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"../models/CatGPT/results\",  # Base directory to save checkpoints\n",
    "    per_device_train_batch_size=24,  # Adjust the batch size per GPU/CPU based on available memory\n",
    "    num_train_epochs=10,  # Start with 10 epochs\n",
    "    save_steps=2000,  # Save model checkpoint every 2000 steps\n",
    "    save_total_limit=1,  # Keep only the latest checkpoint\n",
    "    logging_dir='../models/logs',  # Directory to store logs\n",
    "    logging_steps=500,  # Log the training process every 200 steps\n",
    "    eval_strategy=\"steps\",  # Evaluate the model at every `eval_steps`\n",
    "    eval_steps=500,  # Number of steps between evaluations\n",
    "    load_best_model_at_end=True,  # Load the best model when finished training\n",
    "    metric_for_best_model=\"loss\",  # Metric to determine the best model\n",
    "    greater_is_better=False,  # Minimize the loss (so, not greater is better)\n",
    "    learning_rate=3e-5,  # Learning rate to start with (carefully chosen)\n",
    "    lr_scheduler_type=\"cosine_with_restarts\",  # Use cosine annealing with restarts for learning rate decay\n",
    "    warmup_steps=100,  # Number of warmup steps for the learning rate\n",
    "    weight_decay=0.01,  # Weight decay to avoid overfitting\n",
    "    gradient_accumulation_steps=4,  # Accumulate gradients before updating weights to simulate larger batch size\n",
    "    report_to=\"none\",  # Disable reporting to external services like WandB\n",
    "    fp16=False,  # Enable FP16 for mixed precision training\n",
    ")\n",
    "\n",
    "# Prepare the Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_train_dataset,\n",
    "    eval_dataset=tokenized_eval_dataset,\n",
    ")\n",
    "\n",
    "# Start the training\n",
    "trainer.train()\n",
    "\n",
    "# Save the fine-tuned model\n",
    "trainer.save_model(\"/content/drive/MyDrive/CatGPT/CatGPT/results/fine_tuned_model\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
