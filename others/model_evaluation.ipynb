{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MODEL EVALUATION WITH CATALAN HELLASWAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import torch\n",
    "import json\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import datasets\n",
    "\n",
    "# Load the tokenizer and model\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"baiges/CatGPT\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"baiges/CatGPT\")\n",
    "\n",
    "# Define the device (GPU if available, otherwise CPU)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# Load the Hellaswag dataset translated to Catalan\n",
    "# The dataset is in \"pauhidalgoo/hellaswag-val-ca\"\n",
    "dataset = datasets.load_dataset(\"pauhidalgoo/hellaswag-val-ca\")['train']\n",
    "\n",
    "# Define the evaluation function\n",
    "def evaluate_model(model, tokenizer, dataset, device):\n",
    "    total_correct = 0\n",
    "    total_items = 0\n",
    "\n",
    "    # Iterate over each entry in the dataset\n",
    "    for i, item in tqdm(enumerate(dataset)):\n",
    "        try:\n",
    "            context = item['ctx']  # Get the context\n",
    "            endings = item['endings']  # Get the possible endings\n",
    "            correct_ending_idx = int(item['label'])  # Index of the correct ending\n",
    "\n",
    "            # Tokenize the context and move it to the device\n",
    "            context_input = tokenizer(context, return_tensors=\"pt\").input_ids.to(device).long()\n",
    "\n",
    "            avg_loss_values = []\n",
    "            for ending in endings:\n",
    "                try:\n",
    "                    # Tokenize the ending and move it to the device\n",
    "                    ending_input = tokenizer(ending, return_tensors=\"pt\").input_ids.to(device).long()\n",
    "                    \n",
    "                    # Concatenate the context with each ending\n",
    "                    input_ids = torch.cat([context_input, ending_input[:, 1:]], dim=-1).to(device).long()\n",
    "\n",
    "                    # Calculate the loss only over the ending part\n",
    "                    with torch.no_grad():\n",
    "                        outputs = model(input_ids)\n",
    "                        logits = outputs.logits[:, -ending_input.size(1):, :]  # Get logits corresponding to the ending\n",
    "\n",
    "                        shift_logits = logits[:, :-1, :].contiguous()  # Shift logits to the right\n",
    "                        shift_labels = ending_input[:, 1:].contiguous()  # Shift labels to the left\n",
    "\n",
    "                        # Ensure the logits and labels have the same shape\n",
    "                        shift_logits = shift_logits.view(-1, shift_logits.size(-1))\n",
    "                        shift_labels = shift_labels.view(-1)\n",
    "\n",
    "                        # Calculate loss\n",
    "                        loss_fct = torch.nn.CrossEntropyLoss(reduction='none')\n",
    "                        loss = loss_fct(shift_logits, shift_labels)\n",
    "                        avg_loss = loss.mean().item()  # Calculate the average loss per token\n",
    "                        avg_loss_values.append(avg_loss)\n",
    "                except RuntimeError as e:\n",
    "                    print(f\"Error processing ending: {e}\")\n",
    "                    avg_loss_values.append(float('inf'))  # Consider high loss if there's an error\n",
    "                    continue\n",
    "\n",
    "            # Find the ending with the lowest average loss per token\n",
    "            predicted_idx = torch.argmin(torch.tensor(avg_loss_values)).item()\n",
    "\n",
    "            # Check if the prediction is correct and update the counters\n",
    "            is_correct = predicted_idx == correct_ending_idx\n",
    "            if is_correct:\n",
    "                total_correct += 1\n",
    "            total_items += 1\n",
    "\n",
    "            # Calculate the accuracy for this specific item\n",
    "            item_accuracy = total_correct / total_items\n",
    "\n",
    "            # Print context, prediction, correctness, and accuracy for this item\n",
    "            print(f\"Context: {context}\")\n",
    "            print(f\"Predicted Ending: {endings[predicted_idx]}\")\n",
    "            print(f\"Correct Ending: {endings[correct_ending_idx]}\")\n",
    "            print(f\"Is Correct: {'Yes' if is_correct else 'No'}\")\n",
    "            print(f\"Accuracy so far: {item_accuracy:.4f}\\n\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing item {i}: {e}\")\n",
    "            continue\n",
    "\n",
    "    # Calculate the overall accuracy\n",
    "    accuracy = total_correct / total_items\n",
    "    return accuracy\n",
    "\n",
    "# Perform the evaluation\n",
    "accuracy = evaluate_model(model, tokenizer, dataset, device)\n",
    "print(f\"Final Accuracy: {accuracy:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
